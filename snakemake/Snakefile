import json
from math import *
from os import environ
from os.path import basename, join
import re
import subprocess


#-----------------------------------------------------------------------------
# BEGIN functions
#-----------------------------------------------------------------------------

def rel_to_workdir(path):
    from os.path import relpath

    return relpath(path, config["workdir"])


def db_stub(db_file):
    from os.path import splitext

    return splitext(db_file)[0]


def db_name(db_file):
    from os.path import basename

    return basename(db_stub(db_file))


def prepend_ext(filename, ext):
    from os.path import splitext

    parts = splitext(filename)

    return parts[0] + ext + parts[1]


def fasta_to_workdb(fasta_file, ext):
    return join(config["workdir"], "{db}.{ext}".format(db=db_name(fasta_file), ext=ext))


def fasta2dazz_command(target_db):
    if target_db.endswith(".db"):
        return "fasta2DB"
    else:
        return "fasta2DAM"


def db_files(db):
    from os.path import dirname

    hidden_db_file_suffixes = [".bps", ".idx"]
    hidden_dam_file_suffixes = [".bps", ".hdr", ".idx"]
    root = dirname(db)
    suffixes = hidden_db_file_suffixes if db.endswith(".db") \
                else hidden_dam_file_suffixes

    def __hidden_file(suffix):
        if root:
            return "{}/.{}{}".format(root, db_name(db), suffix)
        else:
            return ".{}{}".format(db_name(db), suffix)

    return [db] + [__hidden_file(suffix)  for suffix in suffixes]


_fast2dam_checkpoint = dict()


def await_db_files(db):
    from os.path import splitext

    def __await_db_files(wildcards):
        if db in _fast2dam_checkpoint:
            _checkpoint = getattr(checkpoints, _fast2dam_checkpoint[db])

            return _checkpoint.get(workdir=workdir_).output
        else:
            raise Exception("unknown db: {}".format(db));

    return __await_db_files


def await_pile_ups():
    def __await_pile_ups(wildcards):
        return checkpoints.collect.get().output

    return __await_pile_ups


def alignment_file(db_a, db_b=None, block_a=None, block_b=None):
    if db_b is None:
        db_b = db_a
    db_a = db_name(db_a)
    db_b = db_name(db_b)

    filename = None
    if block_a is None and block_b is None:
        filename = "{}.{}.las".format(db_a, db_b)
    elif block_a is None:
        filename = "{}.{}.{}.las".format(db_a, db_b, block_b)
    elif block_b is None:
        filename = "{}.{}.{}.las".format(db_a, block_a, db_b)
    else:
        filename = "{}.{}.{}.{}.las".format(db_a, block_a, db_b, block_b)

    return join(config["workdir"], filename)


def shell_esc(string):
    try:
        from shlex import quote
    except ImportError:
        from pipes import quote

    return quote(string)


def make_flags(flags):
    try:
        from shlex import join
    except ImportError:
        try:
            from shlex import quote
        except ImportError:
            from pipes import quote

        def join(split_command):
            return " ".join(quote(cmd)  for cmd in split_command)


    if str(flags) == flags:
        return flags
    else:
        return join(flags)


def append_flags(flags, *new_flags):
    try:
        from shlex import quote
    except ImportError:
        from pipes import quote

    if len(flags) == 0:
        return make_flags(new_flags)
    else:
        return flags + " " + make_flags(new_flags)


def prepare_flags(flags):
    def secondary_expand(wildcards, input=None, output=None, threads=None, resources=None):
        return flags.format(wildcards=wildcards,
                            input=input,
                            output=output,
                            threads=threads,
                            resources=resources)

    return secondary_expand


def ensure_threads_flag(flags):
    from shlex import split

    flags = list(flag  for flag in split(flags) if not flag.startswith("-T"))
    flags.append("-T{threads}")

    return make_flags(flags)


def log_file(step):
    return join(config["logdir"], "{}.log".format(step))


def auxiliary_threads(wildcards, threads):
    if "auxiliary_threads" in config:
        return int(config["auxiliary_threads"])
    else:
        return max(1, threads // 4)


def main_threads(wildcards, threads):
    return threads // auxiliary_threads(wildcards, threads=threads)


def validate_dentist_config(config_file):
    subprocess.check_output(["dentist", "validate-config", config_file])


def generate_options_for(alignment_name, config_file):
    generate_template = "dentist generate --quiet --config={} | sed -nE '/^#.*{}/ {{ n; s/^([^<]+).*/\\1/p }}'"
    generate_cmd = generate_template.format(config_file, alignment_name)

    return subprocess.check_output(generate_cmd, shell=True).decode().rstrip()


def get_num_blocks(db):
    try:
        with open(db, 'r') as db_file:
            for line in db_file:
                if line.startswith("blocks ="):
                    return int(line.rpartition(" ")[2])

        raise EOFError("DB not split: {}".format(db))
    except FileNotFoundError:
        return 1


__num_contigs_regex = re.compile(r"^\+\s+R\s+(?P<num_contigs>\d+)\s*$", re.MULTILINE)


def get_num_contigs(db):
    try:
        dbdump = subprocess.check_output(["DBdump", db]).decode()
        match = __num_contigs_regex.match(dbdump)

        if not match:
            raise Exception("Could not read number of contigs in {}".format(db))

        return int(match.group("num_contigs"))
    except subprocess.CalledProcessError:
        return 1


def block_alignments(db_a, db_b=None, damapper=False):
    num_blocks_a = get_num_blocks(db_a) if not damapper else 0

    if db_b is None:
        db_b = db_a
        num_blocks_b = num_blocks_a
    else:
        num_blocks_b = get_num_blocks(db_b)

    blocks_a = range(1, num_blocks_a + 1)
    blocks_b = range(1, num_blocks_b + 1)

    if damapper:
        return (alignment_file(db_a, db_b, block_b=j)  for j in blocks_b)
    else:
        return (alignment_file(db_a, db_b, i, j)  for i in blocks_a for j in blocks_b)


def mask_files(db, mask):
    from os.path import dirname

    suffixes = ["anno", "data"]
    root = dirname(db)

    def __mask_files(suffix):
        if root:
            return "{}/.{}.{}.{}".format(root, db_name(db), mask, suffix)
        else:
            return ".{}.{}.{}".format(db_name(db), mask, suffix)

    return (__mask_files(suffix)  for suffix in suffixes)


def batch_range(wildcards, batch_size, num_elements):
    batch_id = int(wildcards.batch_id, base=10)
    from_index = batch_id * batch_size
    to_index = min(from_index + batch_size, num_elements)

    return "{}..{}".format(from_index, to_index)


def insertion_batch_range(wildcards):
    return batch_range(wildcards, config["batch_size"], get_num_pile_ups())


def get_num_pile_ups():
    from subprocess import CalledProcessError
    from os.path import exists

    try:
        info_cmd = ["dentist", "show-pile-ups", "-j", pile_ups]
        pile_ups_info = subprocess.check_output(info_cmd, stderr=subprocess.DEVNULL)

        return json.loads(pile_ups_info)["numPileUps"]
    except CalledProcessError as e:
        if exists(pile_ups):
            raise e

        return 1


def ceildiv(n, d):
    return (n + d - 1) // d


def insertions_batches():
    num_pile_ups = get_num_pile_ups()
    num_batches = ceildiv(num_pile_ups, config["batch_size"])
    num_digits = int(ceil(log10(num_batches)))
    batch_id_format = "{{:0{}d}}".format(num_digits)

    def get_insertions_batch(batch_id):
        workdir_ = config["workdir"]
        insertions_batch = config["workflow"]["insertions_batch"]
        batch_id = batch_id_format.format(batch_id)

        return join(workdir_, insertions_batch.format(batch_id=batch_id))

    return (get_insertions_batch(i)  for i in range(num_batches))


def docstring(string):
    paragraph_sep = re.compile(r"\n\s*\n")
    whitespace = re.compile(r"\s+")

    paragraphs = paragraph_sep.split(string)
    paragraphs = (whitespace.sub(" ", par).strip()  for par in paragraphs)

    return "\n\n".join(paragraphs)


def replace_env(string):
    import re

    env_re = re.compile(r"\$(?P<var>\$|[a-zA-Z_][a-zA-Z0-9_]*|\{[a-zA-Z_][a-zA-Z0-9_]*\})")

    def fetch_env(match):
        varname = match["var"]

        if varname == "$":
            return "$"
        elif varname.startswith("{"):
            return environ[varname[1:-1]]
        else:
            return environ[varname]

    return env_re.sub(fetch_env, string)


def replace_env_rec(obj):
    iterable = None

    if isinstance(obj, str):
        return replace_env(obj)
    elif isinstance(obj, list):
        iterable = enumerate(obj)
    elif isinstance(obj, dict):
        iterable = obj.items()
    else:
        return obj

    for k, v in iterable:
        obj[k] = replace_env_rec(v)

    return obj


#-----------------------------------------------------------------------------
# END functions
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# Variables for rules
#-----------------------------------------------------------------------------

env_config = replace_env_rec(config.pop("env", dict()))

if isinstance(env_config, dict):
    for k, v in env_config.items():
        if not k in environ:
            environ[k] = v
else:
    raise Exception("expected dictionary for config[env] but got: " + type(env_config))

config = replace_env_rec(config)

# config shortcuts
inputs = config["inputs"]
outputs = config["outputs"]
workdir_ = config["workdir"]
workflow_ = config["workflow"]
max_threads = config["max_threads"]

# workflow files
reference_fasta = inputs["reference"]
reads_fasta = inputs["reads"]
output_assembly = outputs["output_assembly"]
reference = fasta_to_workdb(reference_fasta, "dam")
reads_db_type = "db" if inputs["reads_type"] == "PACBIO_SMRT" else "dam"
reads = fasta_to_workdb(reads_fasta, reads_db_type)
dentist_config_file = join(workdir_, workflow_["config"])
dentist_merge_config_file = prepend_ext(dentist_config_file, ".merge")
self_alignment = alignment_file(reference)
tandem_alignment = alignment_file("TAN", reference)
ref_vs_reads_alignment = alignment_file(reference, reads)
self_mask = workflow_["self_mask"]
dust_mask = "dust"
tandem_mask = "tan"
reads_mask = workflow_["reads_mask"]
masks = [self_mask, dust_mask, tandem_mask, reads_mask]
pile_ups = join(workdir_, workflow_["pile_ups"])
insertions_batch = join(workdir_, workflow_["insertions_batch"])
insertions = join(workdir_, workflow_["insertions"])

# command-specific
from os import environ
dentist_flags = environ.get("DENTIST_FLAGS", "")
dbsplit_flags = make_flags(config.get("dbsplit_flags", []))
dbdust_flags = make_flags(config.get("dbdust_flags", []))
dalign_flags = ensure_threads_flag(make_flags(config.get("dalign_flags", [])))
datander_flags = ensure_threads_flag(make_flags(config.get("datander_flags", [])))

if "TMPDIR" in environ:
    dalign_flags = append_flags(dalign_flags, "-P{}".format(environ["TMPDIR"]))
    datander_flags = append_flags(datander_flags, "-P{}".format(environ["TMPDIR"]))

lacheck_failed_notice = shell_esc(docstring("""
    Check failed. Possible solutions:

    Duplicate LAs: can be fixed by LAsort from 2020-03-22 or later.

    In order to ignore checks entirely you may use the environment variable
    SKIP_LACHECK=1. Use only if you are positive the files are in fact OK!
"""))


#-----------------------------------------------------------------------------
# BEGIN rules
#-----------------------------------------------------------------------------

localrules:
    ALL,
    extend_dentist_config,
    reference2dam,
    self_alignment,
    reads2db,
    ref_vs_reads_alignment,
    extend_dentist_config_for_merge,
    merge


rule ALL:
    input: output_assembly


rule extend_dentist_config:
    output: dentist_config_file
    run:
        dentist_config = config["dentist_config"]

        if "__default__" not in dentist_config:
            dentist_config["__default__"] = dict()
        dentist_config["__default__"]["reference"] = reference
        dentist_config["__default__"]["reads"] = reads
        dentist_config["__default__"]["result"] = output_assembly
        dentist_config["__default__"]["ref-vs-reads-alignment"] = ref_vs_reads_alignment
        dentist_config["__default__"]["mask"] = masks
        dentist_config["__default__"]["pile-ups"] = pile_ups
        dentist_config["__default__"]["insertions"] = insertions

        if "mask-repetitive-regions" not in dentist_config:
            dentist_config["mask-repetitive-regions"] = dict()
        dentist_config["mask-repetitive-regions"]["reads"] = None

        with open(output[0], 'w') as ext_config_file:
            json.dump(dentist_config, ext_config_file)

        validate_dentist_config(output[0])


_fast2dam_checkpoint[reference] = "reference2dam"
checkpoint reference2dam:
    input: reference_fasta
    output: *db_files(reference)
    shell:
        "fasta2DAM {output[0]} {input} && DBsplit {dbsplit_flags} {output[0]}"


rule mask_dust:
    input:
        dentist_config_file,
        await_db_files(reference)
    output:
        *mask_files(reference, dust_mask)
    params:
        reference_stub = db_stub(reference),
    shell:
        "DBdust {dbdust_flags} {params.reference_stub}"


rule self_alignment_block:
    input:
        dentist_config_file,
        await_db_files(reference),
        mask_files(reference, dust_mask)
    output:
        temp(alignment_file(reference, block_a='{block_ref}', block_b='{block_reads}')),
        temp(alignment_file(reference, block_b='{block_ref}', block_a='{block_reads}'))
    params:
        aligncmd = lambda _: generate_options_for("self", dentist_config_file),
        reference_stub = db_stub(rel_to_workdir(reference)),
        alignment_file_a_vs_b = lambda _, output: basename(output[0]),
        alignment_file_b_vs_a = lambda _, output: basename(output[1]),
        dalign_flags = prepare_flags(dalign_flags)
    threads: max_threads
    log: log_file("self-alignment.{block_ref}.{block_reads}")
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {params.dalign_flags} -m{dust_mask} {params.reference_stub}.{wildcards.block_ref} {params.reference_stub}.{wildcards.block_reads}
                if (( ${{SKIP_LACHECK:-0}} == 0 ))
                then
                    LAcheck -v {params.reference_stub} {params.alignment_file_a_vs_b} || echo {lacheck_failed_notice}
                    LAcheck -v {params.reference_stub} {params.alignment_file_b_vs_a} || echo {lacheck_failed_notice}
                else
                    echo "Skipping LAcheck due to user request"
                fi
            }} &> {log}
        """


rule self_alignment:
    input:
        await_db_files(reference),
        block_alignments = lambda _: block_alignments(reference)
    output:
        protected(self_alignment)
    params:
        alignment_file = lambda _, output: basename(output[0]),
    shell:
        """
            LAmerge {output} {input[block_alignments]}
            if (( ${{SKIP_LACHECK:-0}} == 0 ))
            then
                LAcheck -v {reference} {params.alignment_file} || echo {lacheck_failed_notice}
            else
                echo "Skipping LAcheck due to user request"
            fi
        """


rule mask_self:
    input:
        dentist_config_file,
        await_db_files(reference),
        self_alignment
    output:
        *mask_files(reference, self_mask)
    log: log_file("mask-self")
    shell:
        "dentist mask --config={dentist_config_file} {dentist_flags} {reference} {self_alignment} {self_mask} 2> {log}"


rule tandem_alignment:
    input:
        dentist_config_file,
        await_db_files(reference)
    output:
        tandem_alignment
    params:
        aligncmd = lambda _: generate_options_for("tandem", dentist_config_file),
        reference_stub = db_stub(rel_to_workdir(reference)),
        alignment_file = lambda _, output: basename(output[0]),
        datander_flags = prepare_flags(datander_flags)
    log: log_file("tandem-alignment")
    threads: max_threads
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {params.datander_flags} {params.reference_stub}
                if (( ${{SKIP_LACHECK:-0}} == 0 ))
                then
                    LAcheck -v {params.reference_stub} {params.alignment_file} || echo {lacheck_failed_notice}
                else
                    echo "Skipping LAcheck due to user request"
                fi
            }} &> {log}
        """


rule mask_tandem:
    input:
        dentist_config_file,
        await_db_files(reference),
        tandem_alignment
    output:
        *mask_files(reference, tandem_mask)
    params:
        reference_stub = db_stub(reference),
    shell:
        "TANmask -n{tandem_mask} {params.reference_stub} {tandem_alignment}"


_fast2dam_checkpoint[reads] = "reads2db"
checkpoint reads2db:
    input: reads_fasta
    output: *db_files(reads)
    params:
        fasta2dazz = fasta2dazz_command(reads)
    shell:
        "{params.fasta2dazz} {output[0]} {input} && DBsplit {dbsplit_flags} {output[0]}"


rule ref_vs_reads_alignment_block:
    input:
        dentist_config_file,
        await_db_files(reference),
        lambda _: mask_files(reference, dust_mask),
        lambda _: mask_files(reference, self_mask),
        lambda _: mask_files(reference, tandem_mask),
        await_db_files(reads)
    output: temp(alignment_file(reference, reads, block_b='{block_reads}'))
    params:
        aligncmd = lambda _: generate_options_for("reads", dentist_config_file),
        reference_stub = db_stub(rel_to_workdir(reference)),
        reads_stub = db_stub(rel_to_workdir(reads)),
        alignment_file = lambda _, output: basename(output[0]),
        dalign_flags = prepare_flags(dalign_flags)
    threads: max_threads
    log: log_file("ref-vs-reads-alignment.{block_reads}")
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {params.dalign_flags} -m{dust_mask} -m{self_mask} -m{tandem_mask} {params.reference_stub} {params.reads_stub}.{wildcards.block_reads}
                if (( ${{SKIP_LACHECK:-0}} == 0 ))
                then
                    LAcheck -v {params.reference_stub} {params.reads_stub} {params.alignment_file} || echo {lacheck_failed_notice}
                else
                    echo "Skipping LAcheck due to user request"
                fi
            }} &> {log}
        """


rule ref_vs_reads_alignment:
    input:
        await_db_files(reference),
        await_db_files(reads),
        block_alignments = lambda _: block_alignments(reference, reads, damapper=True)
    output:
        protected(ref_vs_reads_alignment)
    shell:
        "LAmerge {output} {input[block_alignments]}"


rule mask_reads:
    input:
        dentist_config_file,
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_alignment
    output:
        *mask_files(reference, reads_mask)
    log: log_file("mask-reads")
    shell:
        "dentist mask --config={dentist_config_file} {dentist_flags} {reference} {reads} {ref_vs_reads_alignment} {reads_mask} 2> {log}"


checkpoint collect:
    input:
        dentist_config_file,
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_alignment,
        *mask_files(reference, self_mask),
        *mask_files(reference, tandem_mask),
        *mask_files(reference, reads_mask)
    output:
        protected(pile_ups)
    params:
        main_threads = main_threads,
        auxiliary_threads = auxiliary_threads
    threads: max_threads
    log: log_file("collect")
    shell:
        "dentist collect --config={dentist_config_file} {dentist_flags} --threads={params.main_threads} --auxiliary-threads={params.auxiliary_threads} - - - - 2> {log}"


rule process:
    input:
        dentist_config_file,
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_alignment,
        await_pile_ups(),
        mask_files(reference, self_mask),
        mask_files(reference, reads_mask),
    output:
        temp(insertions_batch)
    params:
        batch_range = insertion_batch_range,
        main_threads = main_threads,
        auxiliary_threads = auxiliary_threads
    threads: max_threads
    log: log_file("process.{batch_id}")
    shell:
        "dentist process --config={dentist_config_file} {dentist_flags} --threads={params.main_threads} --auxiliary-threads={params.auxiliary_threads} --batch={params.batch_range} - - - - {output} 2> {log}"

rule extend_dentist_config_for_merge:
    input:
        dentist_config_file,
        await_pile_ups()
    output:
        temp(dentist_merge_config_file)
    run:
        merge_config = None
        with open(dentist_config_file, 'r') as base_config:
            merge_config = json.load(base_config)

        if "merge-insertions" not in merge_config:
            merge_config["merge-insertions"] = dict()
        merge_config["merge-insertions"]["insertions"] = insertions
        merge_config["merge-insertions"]["partitioned-insertions"] = list(insertions_batches())

        with open(output[0], 'w') as ext_config_file:
            json.dump(merge_config, ext_config_file)

        validate_dentist_config(output[0])


rule merge:
    input:
        dentist_merge_config_file,
        await_db_files(reference),
        await_db_files(reads),
        await_pile_ups(),
        insertions_batches = lambda _: insertions_batches()
    output:
        protected(insertions)
    log: log_file("merge")
    shell:
        "dentist merge --config={dentist_merge_config_file} {dentist_flags} - - 2> {log}"

rule output:
    input:
        dentist_config_file,
        await_db_files(reference),
        insertions
    output:
        output_assembly
    log: log_file("output")
    shell:
        "dentist output --config={dentist_config_file} {dentist_flags} - - - 2> {log}"

#-----------------------------------------------------------------------------
# END rules
#-----------------------------------------------------------------------------
