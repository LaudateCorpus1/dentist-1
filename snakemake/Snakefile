import json
from itertools import chain
from math import *
from os import environ
from os.path import basename, exists, join
import re
import shlex
import subprocess


# Declare variables so they are available in functions
# They get populated with proper values later on
workdir_ = None
logdir = None
insertions_batch = None
custom_auxiliary_threads = None
batch_size = None


#-----------------------------------------------------------------------------
# BEGIN functions
#-----------------------------------------------------------------------------

def rel_to_workdir(path):
    from os.path import relpath

    return relpath(path, workdir_)


def db_stub(db_file):
    from os.path import splitext

    return splitext(db_file)[0]


def db_name(db_file):
    from os.path import basename

    return basename(db_stub(db_file))


def prepend_ext(filename, ext):
    from os.path import splitext

    parts = splitext(filename)

    return parts[0] + ext + parts[1]


def fasta_to_workdb(fasta_file, ext):
    return join(workdir_, "{db}.{ext}".format(db=db_name(fasta_file), ext=ext))


def assembly_marker(db):
    return join(workdir_, ".assembly.{db}".format(db=db_name(db)))


def reads_marker(db):
    return join(workdir_, ".reads.{db}".format(db=db_name(db)))


def fasta2dazz_command(target_db):
    if target_db.endswith(".db"):
        return "fasta2DB"
    else:
        return "fasta2DAM"


def db_files(db):
    from os.path import dirname

    hidden_db_file_suffixes = [".bps", ".idx"]
    hidden_dam_file_suffixes = [".bps", ".hdr", ".idx"]
    root = dirname(db)
    suffixes = hidden_db_file_suffixes if db.endswith(".db") \
                else hidden_dam_file_suffixes

    def __hidden_file(suffix):
        if root:
            return "{}/.{}{}".format(root, db_name(db), suffix)
        else:
            return ".{}{}".format(db_name(db), suffix)

    return [db] + [__hidden_file(suffix)  for suffix in suffixes]


class IgnoreMissingFormat(dict):
    def __missing__(self, key):
        return "{" + key + "}"


def expand_wildcards(string, wildcards, more_attributes={}):
    format_dict = IgnoreMissingFormat(wildcards)

    for key, value in more_attributes.items():
        if not key in format_dict:
            format_dict[key] = value

    return string.format_map(format_dict)


_fast2dam_checkpoint = dict()


def await_db_files(db):
    from os.path import splitext

    def __await_db_files(wildcards):
        expanded_db = expand_wildcards(db, wildcards)

        if expanded_db in _fast2dam_checkpoint:
            _checkpoint = getattr(checkpoints, _fast2dam_checkpoint[expanded_db])

            return _checkpoint.get(workdir=workdir_).output
        else:
            raise Exception("unknown db: {}".format(expanded_db));

    return __await_db_files


def await_dentist_config():
    def __await_dentist_config(wildcards):
        return checkpoints.extend_dentist_config.get().output

    return __await_dentist_config


def await_pile_ups():
    def __await_pile_ups(wildcards):
        return checkpoints.collect.get().output

    return __await_pile_ups


def alignment_file(db_a, db_b=None, block_a=None, block_b=None):
    if db_b is None:
        db_b = db_a
    db_a = db_name(db_a)
    db_b = db_name(db_b)

    filename = None
    if block_a is None and block_b is None:
        filename = "{}.{}.las".format(db_a, db_b)
    elif block_a is None:
        filename = "{}.{}.{}.las".format(db_a, db_b, block_b)
    elif block_b is None:
        filename = "{}.{}.{}.las".format(db_a, block_a, db_b)
    else:
        filename = "{}.{}.{}.{}.las".format(db_a, block_a, db_b, block_b)

    return join(workdir_, filename)


def shell_esc(string):
    try:
        from shlex import quote
    except ImportError:
        from pipes import quote

    return quote(string)


def make_flags(flags):
    try:
        from shlex import join
    except ImportError:
        try:
            from shlex import quote
        except ImportError:
            from pipes import quote

        def join(split_command):
            return " ".join(quote(cmd)  for cmd in split_command)


    if str(flags) == flags:
        return flags
    else:
        return join(flags)


def append_flags(flags, *new_flags):
    try:
        from shlex import quote
    except ImportError:
        from pipes import quote

    if len(flags) == 0:
        return make_flags(new_flags)
    else:
        return flags + " " + make_flags(new_flags)


def prepare_flags(flags):
    def secondary_expand(wildcards, input=None, output=None, threads=None, resources=None):
        txtflags = flags if not callable(flags) else flags()

        return txtflags.format(wildcards=wildcards,
                            input=input,
                            output=output,
                            threads=threads,
                            resources=resources)

    return secondary_expand


def ensure_threads_flag(flags):
    return ensure_flags(flags, { "-T": "{threads}" })


def ensure_flags(flags, flags_with_values):
    from shlex import split

    flags = list(f  for f in split(flags) if not f[0:2] in flags_with_values)

    for flag, value in flags_with_values.items():
        if value == True:
            flags.append(flag)
        elif value:
            flags.append(flag + str(value))

    return make_flags(flags)


def assert_flag(flags, required_flag, message="required flag is missing: {missing_flags}"):
    assert_flags(flags, [required_flag], message)


def assert_flags(flags, required_flags, message="required flag(s) are missing: {missing_flags}"):
    from shlex import split

    present_flags = dict(((f[0:2], True)  for f in split(flags)))

    missing_flags = list()
    for required_flag in required_flags:
        if not present_flags.get(required_flag, False):
            missing_flags.append(required_flag)

    if len(missing_flags) > 0:
        e = AssertionError(message.format(missing_flags=", ".join(missing_flags)))
        e.missing_flags = missing_flags

        raise e


def log_file(step):
    return join(logdir, "{}.log".format(step))


def auxiliary_threads(wildcards, threads):
    if not custom_auxiliary_threads is None:
        return custom_auxiliary_threads
    else:
        return max(1, threads // 4)


def main_threads(wildcards, threads):
    return threads // auxiliary_threads(wildcards, threads=threads)


def validate_dentist_config(config_file):
    subprocess.check_output(["dentist", "validate-config", config_file])


def generate_options_for(alignment_name, config_file):
    if not exists(config_file):
        return "false"

    generate_template = "dentist generate --quiet --config={}"
    generate_cmd = generate_template.format(config_file, shell_esc(alignment_name))
    generate_proc = subprocess.run(generate_cmd,
                                   shell=True,
                                   stderr=subprocess.PIPE,
                                   stdout=subprocess.PIPE)
    commands = generate_proc.stdout.decode().splitlines()

    if generate_proc.returncode != 0 or len(commands) == 0:
        verbose_generate_cmd = generate_cmd.replace("--quiet", "-v -v")
        error = generate_proc.stderr.decode()

        raise Exception("failed to get alignment commands: " + error)

    while not commands[0].startswith("#"):
        commands = commands[1:]

    for comment, command in zip(commands[::2], commands[1::2]):
        if alignment_name in comment:
            command = re.sub(r"<[^>]+>", "", command)
            command = command.rstrip()

            return command

    raise Exception("failed to get alignment command: unknown alignment_name `{}`".format(alignment_name))


def get_num_blocks(db):
    try:
        with open(db, 'r') as db_file:
            for line in db_file:
                if line.startswith("blocks ="):
                    return int(line.rpartition(" ")[2])

        raise EOFError("DB not split: {}".format(db))
    except FileNotFoundError:
        return 1


__num_contigs_regex = re.compile(r"^\+\s+R\s+(?P<num_contigs>\d+)\s*$", re.MULTILINE)


def get_num_contigs(db):
    try:
        dbdump = subprocess.check_output(["DBdump", db]).decode()
        match = __num_contigs_regex.match(dbdump)

        if not match:
            raise Exception("Could not read number of contigs in {}".format(db))

        return int(match.group("num_contigs"))
    except subprocess.CalledProcessError:
        return 1


def block_alignments(db_a, db_b=None, damapper=None):
    if db_b is None:
        db_b = db_a

    num_blocks_a = get_num_blocks(db_a)
    num_blocks_b = get_num_blocks(db_b)

    blocks_a = range(1, num_blocks_a + 1)
    blocks_b = range(1, num_blocks_b + 1)

    if damapper is None:
        return (alignment_file(db_a, db_b, i, j)  for i in blocks_a for j in blocks_b)
    elif damapper == "forward":
        return (alignment_file(db_a, db_b, block_b=j)  for j in blocks_b)
    elif damapper == "reverse":
        return (alignment_file(db_a, db_b, block_a=i)  for i in blocks_a)
    else:
        raise Exception("illegal value for damapper: " + repr(damapper))


def homogenized_mask(mask):
    return mask + "-H"


def block_mask(mask, block):
    return "{}.{}".format(block, mask)


def block_masks(mask, db):
    return [block_mask(mask, b + 1)  for b in range(get_num_blocks(db))]


def pseudo_block_mask(mask, block):
    return "{}-{}B".format(mask, block)


def pseudo_block_masks(mask, db):
    return [pseudo_block_mask(mask, b + 1)  for b in range(get_num_blocks(db))]


def mask_files(db, mask):
    from os.path import dirname

    suffixes = ["anno", "data"]
    root = dirname(db)

    def __mask_files(suffix):
        if root:
            return "{}/.{}.{}.{}".format(root, db_name(db), mask, suffix)
        else:
            return ".{}.{}.{}".format(db_name(db), mask, suffix)

    return (__mask_files(suffix)  for suffix in suffixes)


def batch_range(wildcards, batch_size, num_elements):
    batch_id = int(wildcards.batch_id, base=10)
    from_index = batch_id * batch_size
    to_index = min(from_index + batch_size, num_elements)

    return "{}..{}".format(from_index, to_index)


def insertion_batch_range(wildcards):
    return batch_range(wildcards, batch_size, get_num_pile_ups())


def get_num_pile_ups():
    from subprocess import CalledProcessError
    from os.path import exists

    try:
        info_cmd = ["dentist", "show-pile-ups", "-j", pile_ups]
        pile_ups_info = subprocess.check_output(info_cmd, stderr=subprocess.DEVNULL)

        return json.loads(pile_ups_info)["numPileUps"]
    except CalledProcessError as e:
        if exists(pile_ups):
            raise e

        return 1


def ceildiv(n, d):
    return (n + d - 1) // d


def insertions_batches():
    num_pile_ups = get_num_pile_ups()
    num_batches = ceildiv(num_pile_ups, batch_size)
    num_digits = int(ceil(log10(num_batches)))
    batch_id_format = "{{:0{}d}}".format(num_digits)

    def get_insertions_batch(batch_id):
        batch_id = batch_id_format.format(batch_id)

        return join(workdir_, insertions_batch.format(batch_id=batch_id))

    return (get_insertions_batch(i)  for i in range(num_batches))


def docstring(string):
    paragraph_sep = re.compile(r"\n\s*\n")
    whitespace = re.compile(r"\s+")

    paragraphs = paragraph_sep.split(string)
    paragraphs = (whitespace.sub(" ", par).strip()  for par in paragraphs)

    return "\n\n".join(paragraphs)


def replace_env(string, envdict=environ):
    import re

    env_re = re.compile(r"\$(?P<var>\$|[a-zA-Z_][a-zA-Z0-9_]*|\{[a-zA-Z_][a-zA-Z0-9_]*\})")

    def fetch_env(match):
        varname = match["var"]

        if varname == "$":
            return "$"
        elif varname.startswith("{"):
            return envdict[varname[1:-1]]
        else:
            return envdict[varname]

    fullmatch = env_re.fullmatch(string)
    if fullmatch:
        return fetch_env(fullmatch)
    else:
        return env_re.sub(lambda m: str(fetch_env(m)), string)


def replace_env_rec(obj, envdict=environ):
    iterable = None

    if isinstance(obj, str):
        return replace_env(obj, envdict)
    elif isinstance(obj, list):
        iterable = enumerate(obj)
    elif isinstance(obj, dict):
        iterable = obj.items()
    else:
        return obj

    for k, v in iterable:
        obj[k] = replace_env_rec(v, envdict)

    return obj


def multi_json_loads(s, **kwargs):
    """Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance
       containing whitespace-separated JSON documents) to a list of Python
       objects.
    """

    decoder = json.JSONDecoder(**kwargs)

    start = len(s) - len(s.lstrip())
    docs = list()
    while start < len(s):
        doc, pos = decoder.raw_decode(s, idx=start)
        docs.append(doc)
        lsep = len(s[pos:]) - len(s[pos:].lstrip())

        if lsep == 0:
            raise json.JSONDecodeError("Extra data", s, pos)

        start = pos + lsep

    return docs


def multi_json_load(fp, **kwargs):
    """Deserialize ``fp`` (a ``.read()``-supporting file-like object containing
       containing whitespace-separated JSON documents) to a list of Python
       objects.
    """

    return multi_json_loads(fp.read(), **kwargs)


#-----------------------------------------------------------------------------
# END functions
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# Variables for rules
#-----------------------------------------------------------------------------

augmented_env = replace_env_rec(config.pop("default_env", dict()))

if isinstance(augmented_env, dict):
    for k, v in environ.items():
        augmented_env[k] = v
else:
    raise Exception("expected dictionary for config[default_env] but got: " + type(augmented_env))

override_env = replace_env_rec(config.pop("override_env", dict()))
if isinstance(override_env, dict):
    for k, v in override_env.items():
        augmented_env[k] = v
else:
    raise Exception("expected dictionary for config[override_env] but got: " + type(override_env))

for k, v in augmented_env.items():
    environ[k] = str(v)

config = replace_env_rec(config, augmented_env)

# config shortcuts
inputs = config["inputs"]
outputs = config["outputs"]
workdir_ = config["workdir"]
logdir = config["logdir"]
workflow_ = config.get("workflow", {})
max_threads = config["max_threads"]
custom_auxiliary_threads = config.get("auxiliary_threads", None)
batch_size = config["batch_size"]
validation_blocks = config["validation_blocks"]
workflow_flags_names = ["debug"]
workflow_flags = dict(((flag, config.get(flag, False))  for flag in workflow_flags_names))


# workflow files
reference_fasta = inputs["reference"]
reads_fasta = inputs["reads"]
gap_closed_fasta = outputs["output_assembly"]
preliminary_gap_closed_fasta = join(workdir_, db_name(outputs["output_assembly"]) + "-preliminary.fasta")
validation_report = outputs.get("validation_report", join(workdir_, "validation-report.json"))
validation_report_block = join(workdir_, "validation-report.{block_ref}.json")
gap_closed = fasta_to_workdb(gap_closed_fasta, "dam")
preliminary_gap_closed = fasta_to_workdb(preliminary_gap_closed_fasta, "dam")
closed_gaps_mask = workflow_.get("closed_gaps_mask", "closed-gaps")
preliminary_closed_gaps_bed = join(workdir_, "{}.{}.bed".format(db_name(preliminary_gap_closed), closed_gaps_mask))
closed_gaps_bed = join(workdir_, "{}.{}.bed".format(db_name(gap_closed), closed_gaps_mask))
reference = fasta_to_workdb(reference_fasta, "dam")
reads_db_type = "db" if inputs["reads_type"] == "PACBIO_SMRT" else "dam"
reads = fasta_to_workdb(reads_fasta, reads_db_type)
dentist_config_file = join(workdir_, workflow_.get("config", "dentist.json"))
dentist_merge_insertions_config_file = prepend_ext(dentist_config_file, ".merge")
skip_gaps = join(workdir_, workflow_.get("skip-gaps", "skip-gaps.txt"))
self_alignment = alignment_file(reference)
tandem_alignment = alignment_file("TAN", reference)
tandem_alignment_block = alignment_file("TAN", reference, block_b="{block}")
ref_vs_reads_alignment = alignment_file(reference, reads)
reads_vs_ref_alignment = alignment_file(reads, reference)
chained_ext = ".chained"
ref_vs_reads_chained_alignment = prepend_ext(ref_vs_reads_alignment, chained_ext)
preliminary_gap_closed_vs_reads_alignment = alignment_file(preliminary_gap_closed, reads)
self_mask = workflow_.get("self_mask", "dentist-self")
dust_mask = "dust"
tandem_mask = "tan"
reads_mask = workflow_.get("reads_mask", "dentist-reads")
weak_coverage_mask = workflow_.get("weak_coverage_mask", "dentist-weak-coverage")
masks = [
    self_mask,
    tandem_mask,
    reads_mask,
]
pile_ups = join(workdir_, workflow_.get("pile_ups", "pile-ups.db"))
insertions_batch = join(workdir_, workflow_.get("insertions_batch", "insertions/batch.{batch_id}.db"))
insertions = join(workdir_, workflow_.get("insertions", "insertions.db"))

# command-specific
from os import environ
dentist_flags = environ.get("DENTIST_FLAGS", "")

additional_options = config.get("additional_options", {})
additional_reference_dbsplit_options = make_flags(additional_options.get("reference_dbsplit", []))
assert_flag(
    additional_reference_dbsplit_options,
    "-x",
    "DBsplit should have a -x flag; provide at minimum -x20 to avoid errors",
)
additional_reads_dbsplit_options = make_flags(additional_options.get("reads_dbsplit", []))
assert_flag(
    additional_reads_dbsplit_options,
    "-x",
    "DBsplit should have a -x flag; provide at minimum -x20 to avoid errors",
)
additional_tanmask_options = make_flags(config.get("tanmask", []))
additional_tanmask_options = ensure_flags(additional_tanmask_options, {
    "-v": True,
    "-n": tandem_mask,
})

additional_self_alignment_options = make_flags(additional_options.get("self_alignment", []))
additional_tandem_alignment_options = make_flags(additional_options.get("tandem_alignment", []))
additional_ref_vs_reads_alignment_options = make_flags(additional_options.get("ref_vs_reads_alignment", []))
additional_reads_vs_reads_alignment_options = make_flags(additional_options.get("reads_vs_reads_alignment", []))
additional_consensus_alignment_options = make_flags(additional_options.get("consensus_alignment", []))
additional_consensus_options = make_flags(additional_options.get("consensus", []))
additional_reference_dust_options = make_flags(additional_options.get("reference_dust", []))
additional_reads_dust_options = make_flags(additional_options.get("reads_dust", []))

if "TMPDIR" in environ:
    tmpdir_flag = { "-P": environ["TMPDIR"] }

    additional_self_alignment_options = ensure_flags(additional_self_alignment_options, tmpdir_flag)
    additional_tandem_alignment_options = ensure_flags(additional_tandem_alignment_options, tmpdir_flag)
    additional_ref_vs_reads_alignment_options = ensure_flags(additional_ref_vs_reads_alignment_options, tmpdir_flag)
    additional_reads_vs_reads_alignment_options = ensure_flags(additional_reads_vs_reads_alignment_options, tmpdir_flag)
    additional_consensus_alignment_options = ensure_flags(additional_consensus_alignment_options, tmpdir_flag)

lacheck_failed_notice = shell_esc(docstring("""
    Check failed. Possible solutions:

    Duplicate LAs: can be fixed by LAsort from 2020-03-22 or later.

    In order to ignore checks entirely you may use the environment variable
    SKIP_LACHECK=1. Use only if you are positive the files are in fact OK!
"""))

lacheck_handler = "{ echo " + lacheck_failed_notice + "; (( ${SKIP_LACHECK:-0} != 0 )); }"

#-----------------------------------------------------------------------------
# BEGIN rules
#-----------------------------------------------------------------------------

localrules:
    ALL,
    extend_dentist_config,
    reference2dam,
    mark_assembly_reference,
    preliminary_gap_closed2dam,
    mark_assembly_preliminary_gap_closed,
    tandem_alignment,
    mask_tandem,
    self_alignment,
    reads2db,
    mark_reads,
    ref_vs_reads_alignment,
    reads_vs_ref_alignment,
    ref_vs_reads_chained_alignment,
    propagate_mask_back_to_reference,
    extend_dentist_config_for_merge_insertions,
    merge_insertions,
    preliminary_gap_closed_vs_reads_alignment,
    split_preliminary_gap_closed_vs_reads_alignment,
    preliminary_closed_gaps_bed2mask,
    validate_regions,
    weak_coverage_mask,
    skip_gaps


wildcard_constraints:
    dam = r"[^/\.]+",
    block_reads = r"[0-9]+",
    block_ref = r"[0-9]+",
    block = r"[0-9]+"


default_targets = [
    gap_closed_fasta,
    closed_gaps_bed,
]

if workflow_flags["debug"]:
    default_targets.append([
        validation_report,
        mask_files(preliminary_gap_closed, weak_coverage_mask),
    ])


rule ALL:
    input: default_targets


checkpoint extend_dentist_config:
    output: dentist_config_file
    run:
        dentist_config = config["dentist_config"]

        if "__default__" not in dentist_config:
            dentist_config["__default__"] = dict()
        dentist_config["__default__"]["reference"] = reference
        dentist_config["__default__"]["reads"] = reads
        dentist_config["__default__"]["result"] = gap_closed_fasta
        dentist_config["__default__"]["ref-vs-reads-alignment"] = ref_vs_reads_chained_alignment
        dentist_config["__default__"]["mask"] = [homogenized_mask(m)  for m in masks]
        dentist_config["__default__"]["pile-ups"] = pile_ups
        dentist_config["__default__"]["insertions"] = insertions
        dentist_config["__default__"]["daccord"] = shlex.split(additional_consensus_options)
        dentist_config["__default__"]["daligner-consensus"] = shlex.split(additional_consensus_alignment_options)
        dentist_config["__default__"]["daligner-reads-vs-reads"] = shlex.split(additional_reads_vs_reads_alignment_options)
        dentist_config["__default__"]["daligner-ref-vs-reads"] = shlex.split(additional_ref_vs_reads_alignment_options)
        dentist_config["__default__"]["daligner-self"] = shlex.split(additional_self_alignment_options)
        dentist_config["__default__"]["datander-ref"] = shlex.split(additional_tandem_alignment_options)
        dentist_config["__default__"]["dust-reads"] = shlex.split(additional_reads_dust_options)


        if "mask-repetitive-regions" not in dentist_config:
            dentist_config["mask-repetitive-regions"] = dict()
        dentist_config["mask-repetitive-regions"]["reads"] = None

        if "bed2mask" not in dentist_config:
            dentist_config["bed2mask"] = dict()
        dentist_config["bed2mask"]["mask"] = None

        if "propagate-mask" not in dentist_config:
            dentist_config["propagate-mask"] = dict()
        dentist_config["propagate-mask"]["mask"] = []

        if "output" not in dentist_config:
            dentist_config["output"] = dict()
        dentist_config["output"]["closed-gaps-bed"] = closed_gaps_bed

        if "validate-regions" not in dentist_config:
            dentist_config["validate-regions"] = dict()
        dentist_config["validate-regions"]["reference"] = preliminary_gap_closed
        dentist_config["validate-regions"]["regions"] = closed_gaps_mask

        with open(output[0], 'w') as ext_config_file:
            json.dump(dentist_config, ext_config_file)

        validate_dentist_config(output[0])


_fast2dam_checkpoint[reference] = "reference2dam"
checkpoint reference2dam:
    input: reference_fasta
    output: *db_files(reference)
    shell:
        "fasta2DAM {output[0]} {input} && DBsplit {additional_reference_dbsplit_options} {output[0]}"


rule mark_assembly_reference:
    input: reference_fasta
    output: touch(assembly_marker(reference))


_fast2dam_checkpoint[preliminary_gap_closed] = "preliminary_gap_closed2dam"
checkpoint preliminary_gap_closed2dam:
    input: preliminary_gap_closed_fasta
    output: *db_files(preliminary_gap_closed)
    shell:
        "fasta2DAM {output[0]} {input} && DBsplit {additional_reference_dbsplit_options} {output[0]}"


rule mark_assembly_preliminary_gap_closed:
    input: preliminary_gap_closed_fasta
    output: touch(assembly_marker(preliminary_gap_closed))


rule mask_dust:
    input:
        await_dentist_config(),
        await_db_files(join(workdir_, "{dam}.dam")),
        assembly_marker("{dam}.dam")
    output:
        mask_files(join(workdir_, "{dam}.dam"), dust_mask)
    shell:
        "DBdust {additional_reference_dust_options} {workdir_}/{wildcards.dam}"


rule self_alignment_block:
    input:
        await_dentist_config(),
        await_db_files(join(workdir_, "{dam}.dam")),
        assembly_marker("{dam}.dam"),
        mask_files(join(workdir_, "{dam}.dam"), dust_mask)
    output:
        temp(alignment_file("{dam}", block_a='{block_ref}', block_b='{block_reads}')),
        temp(alignment_file("{dam}", block_b='{block_ref}', block_a='{block_reads}'))
    params:
        aligncmd = prepare_flags(lambda : generate_options_for("self", dentist_config_file)),
        alignment_file_a_vs_b = lambda _, output: basename(output[0]),
        alignment_file_b_vs_a = lambda _, output: basename(output[1])
    threads: max_threads
    log: log_file("self-alignment.{dam}.{block_ref}.{block_reads}")
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {wildcards.dam}.{wildcards.block_ref} {wildcards.dam}.{wildcards.block_reads}

                LAcheck -v {wildcards.dam} {params.alignment_file_a_vs_b} || {lacheck_handler}
                LAcheck -v {wildcards.dam} {params.alignment_file_b_vs_a} || {lacheck_handler}
            }} &> {log}
        """


rule self_alignment:
    input:
        db = await_db_files(join(workdir_, "{dam}.dam")),
        marker = assembly_marker("{dam}.dam"),
        block_alignments = block_alignments(join(workdir_, "{dam}.dam"))
    output:
        protected(alignment_file("{dam}"))
    log: log_file("self-alignment.{dam}")
    shell:
        """
            {{
                LAmerge {output} {input[block_alignments]}

                LAcheck -v {input.db[0]} {output} || {lacheck_handler}
            }} &> {log}
        """


rule mask_self:
    input:
        await_dentist_config(),
        dam = await_db_files(join(workdir_, "{dam}.dam")),
        marker = assembly_marker("{dam}.dam"),
        alignment = alignment_file("{dam}")
    output:
        mask_files(join(workdir_, "{dam}.dam"), self_mask)
    log: log_file("mask-self.{dam}")
    shell:
        "dentist mask --config={dentist_config_file} {dentist_flags} {input.dam[0]} {input.alignment} {self_mask} 2> {log}"


rule tandem_alignment_block:
    input:
        await_dentist_config(),
        await_db_files(join(workdir_, "{dam}.dam")),
        assembly_marker("{dam}.dam")
    output:
        temp(alignment_file("TAN", "{dam}", block_b="{block}"))
    params:
        aligncmd = prepare_flags(lambda : generate_options_for("tandem", dentist_config_file)),
        alignment_file = lambda _, output: basename(output[0])
    log: log_file("tandem-alignment.{dam}.{block}")
    threads: max_threads
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {wildcards.dam}.{wildcards.block}
                LAcheck -v {wildcards.dam} {params.alignment_file} || {lacheck_handler}
            }} &> {log}
        """


rule tandem_alignment:
    input:
        db = await_db_files(join(workdir_, "{dam}.dam")),
        marker = assembly_marker("{dam}.dam"),
        block_alignments = lambda wildcards: block_alignments("TAN", join(workdir_, wildcards.dam + ".dam"), damapper="forward")
    output:
        protected(alignment_file("TAN", "{dam}"))
    log: log_file("tandem-alignment.{dam}")
    shell:
        """
            {{
                LAmerge {output} {input[block_alignments]}
                LAcheck -v {input.db[0]} {output} || {lacheck_handler}
            }} &> {log}
        """


rule mask_tandem_block:
    input:
        await_dentist_config(),
        await_db_files(join(workdir_, "{dam}.dam")),
        assembly_marker("{dam}.dam"),
        alignment = alignment_file("TAN", "{dam}", block_b="{block}")
    output:
        temp(mask_files(join(workdir_, "{dam}.dam"), block_mask(tandem_mask, "{block}")))
    params:
        reference_stub = join(workdir_, "{dam}"),
        tanmask_options = prepare_flags(additional_tanmask_options)
    log: log_file("mask-tandem.{dam}.{block}")
    shell:
        "TANmask {params[tanmask_options]} {params.reference_stub} {input[alignment]} &> {log}"


rule mask_tandem:
    input:
        await_dentist_config(),
        db = await_db_files(join(workdir_, "{dam}.dam")),
        marker = assembly_marker("{dam}.dam"),
        mask_files = lambda wildcards: chain(*(mask_files(join(workdir_, "{dam}.dam"), m)  for m in block_masks(tandem_mask, join(workdir_, wildcards.dam + ".dam"))))
    output:
        mask_files(join(workdir_, "{dam}.dam"), tandem_mask)
    log: log_file("mask-tandem.{dam}")
    shell:
        "Catrack -v {input.db[0]} {tandem_mask} &> {log}"


_fast2dam_checkpoint[reads] = "reads2db"
checkpoint reads2db:
    input: reads_fasta
    output: *db_files(reads)
    params:
        fasta2dazz = fasta2dazz_command(reads)
    shell:
        "{params.fasta2dazz} {output[0]} {input} && DBsplit {additional_reads_dbsplit_options} {output[0]}"


rule mark_reads:
    input: reads_fasta
    output: touch(reads_marker(reads))


rule ref_vs_reads_alignment_block:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        mask_files(reference, dust_mask),
        mask_files(reference, self_mask),
        mask_files(reference, tandem_mask)
    output:
        alignment_file(reference, reads, block_b='{block_reads}'),
        alignment_file(reads, reference, block_a='{block_reads}')
    params:
        aligncmd = prepare_flags(lambda : generate_options_for("reads", dentist_config_file)),
        reference_stub = db_stub(rel_to_workdir(reference)),
        reads_stub = db_stub(rel_to_workdir(reads)),
        alignment_file_a_vs_b = lambda _, output: basename(output[0]),
        alignment_file_b_vs_a = lambda _, output: basename(output[1]),
        dalign_flags = prepare_flags(additional_reads_vs_reads_alignment_options)
    threads: max_threads
    log: log_file("ref-vs-reads-alignment.{block_reads}")
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {params.dalign_flags} {params.reference_stub} {params.reads_stub}.{wildcards.block_reads}
                LAcheck -v {params.reference_stub} {params.reads_stub} {params.alignment_file_a_vs_b} || {lacheck_handler}
                LAcheck -v {params.reads_stub} {params.reference_stub} {params.alignment_file_b_vs_a} || {lacheck_handler}
            }} &> {log}
        """


rule ref_vs_reads_alignment:
    input:
        await_db_files(reference),
        await_db_files(reads),
        block_alignments = lambda _: block_alignments(reference, reads, damapper="forward")
    output:
        protected(ref_vs_reads_alignment)
    log: log_file("ref-vs-reads-alignment")
    shell:
        """
            {{
                LAmerge {output} {input[block_alignments]}
                LAcheck -v {reference} {reads} {output} || {lacheck_handler}
            }} &> {log}
        """


rule reads_vs_ref_alignment:
    input:
        await_db_files(reference),
        await_db_files(reads),
        block_alignments = lambda _: block_alignments(reads, reference, damapper="reverse")
    output:
        protected(reads_vs_ref_alignment)
    shell:
        "LAmerge {output} {input[block_alignments]}"


rule ref_vs_reads_chained_alignment_block:
    input:
        config = await_dentist_config(),
        refdb = await_db_files(reference),
        readsdb = await_db_files(reads),
        alignment = alignment_file(reference, reads, block_b='{block_reads}')
    output:
        prepend_ext(alignment_file(reference, reads, block_b='{block_reads}'), chained_ext)
    params:
        reference_stub = db_stub(reference),
        reads_stub = db_stub(reads)
    threads: max_threads
    log: log_file("ref-vs-reads-chained-alignment.{block_reads}")
    shell:
        """
            {{
                dentist chain-local-alignments --config={input.config} {input.refdb[0]} {input.readsdb[0]} {input.alignment} {output}
                LAcheck -v {input.refdb[0]} {input.readsdb[0]} {output} || {lacheck_handler}
            }} &> {log}
        """


rule ref_vs_reads_chained_alignment:
    input:
        await_db_files(reference),
        await_db_files(reads),
        block_alignments = lambda _: [prepend_ext(blas, chained_ext)  for blas in block_alignments(reference, reads, damapper="forward")]
    output:
        protected(ref_vs_reads_chained_alignment)
    log: log_file("ref-vs-reads-chained-alignment")
    shell:
        """
            {{
                LAmerge {output} {input[block_alignments]}
                LAcheck -v {reference} {reads} {output} || {lacheck_handler}
            }} &> {log}
        """


rule mask_reads:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_alignment
    output:
        mask_files(reference, reads_mask)
    log: log_file("mask-reads")
    shell:
        "dentist mask --config={dentist_config_file} {dentist_flags} {reference} {reads} {ref_vs_reads_alignment} {reads_mask} 2> {log}"


rule propagate_mask_to_reads_block:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        mask_files(reference, "{mask}"),
        alignment = alignment_file(reference, reads, block_b='{block_reads}')
    output:
        mask_files(reads, block_mask("{mask}", "{block_reads}"))
    params:
        inmask = "{mask}",
        outmask = block_mask("{mask}", "{block_reads}")
    log: log_file("propagate-mask-to-reads.{mask}.{block_reads}")
    shell:
        "dentist propagate-mask --config={dentist_config_file} {dentist_flags} -m {params[inmask]} {reference} {reads} {input[alignment]} {params[outmask]} 2> {log}"


# rule propagate_mask_to_reads:
#     input:
#         await_dentist_config(),
#         await_db_files(reads),
#         *[mask_files(reads, m) for m in block_masks("{mask}", reads)]
#     output:
#         mask_files(reads, "{mask}")
#     params:
#         merged_mask = "{mask}",
#         block_masks = block_masks("{mask}", reads)
#     log: log_file("propagate-mask-to-reads.{mask}")
#     shell:
#         "dentist merge-masks --config={dentist_config_file} {dentist_flags} {reads} {params[merged_mask]} {params[block_masks]} 2> {log}"


rule propagate_mask_back_to_reference_block:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        mask_files(reads, block_mask("{mask}", "{block_reads}")),
        alignment = alignment_file(reads, reference, block_a='{block_reads}')
    output:
        temp(mask_files(reference, pseudo_block_mask(homogenized_mask("{mask}"), "{block_reads}")))
    params:
        inmask = block_mask("{mask}", "{block_reads}"),
        outmask = pseudo_block_mask(homogenized_mask("{mask}"), "{block_reads}")
    log: log_file("propagate-mask-back-to-reference.{mask}.{block_reads}")
    shell:
        "dentist propagate-mask --config={dentist_config_file} {dentist_flags} -m {params[inmask]} {reads} {reference} {input[alignment]} {params[outmask]} 2> {log}"


rule propagate_mask_back_to_reference:
    input:
        await_dentist_config(),
        await_db_files(reference),
        *[mask_files(reference, m) for m in pseudo_block_masks(homogenized_mask("{mask}"), reads)]
    output:
        mask_files(reference, homogenized_mask("{mask}"))
    params:
        merged_mask = homogenized_mask("{mask}"),
        block_masks = pseudo_block_masks(homogenized_mask("{mask}"), reads)
    log: log_file("propagate-mask-back-to-reference.{mask}")
    shell:
        "dentist merge-masks --config={dentist_config_file} {dentist_flags} {reference} {params[merged_mask]} {params[block_masks]} 2> {log}"


checkpoint collect:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_chained_alignment,
        *[mask_files(reference, homogenized_mask(m))  for m in masks]
    output:
        protected(pile_ups)
    params:
        main_threads = main_threads,
        auxiliary_threads = auxiliary_threads
    threads: max_threads
    log: log_file("collect")
    shell:
        "dentist collect --config={dentist_config_file} {dentist_flags} --threads={params.main_threads} --auxiliary-threads={params.auxiliary_threads} - - - - 2> {log}"


rule process:
    input:
        await_dentist_config(),
        await_db_files(reference),
        await_db_files(reads),
        ref_vs_reads_alignment,
        await_pile_ups(),
        *[mask_files(reference, homogenized_mask(m))  for m in masks]
    output:
        temp(insertions_batch)
    params:
        batch_range = insertion_batch_range,
        main_threads = main_threads,
        auxiliary_threads = auxiliary_threads
    threads: max_threads
    log: log_file("process.{batch_id}")
    shell:
        "dentist process --config={dentist_config_file} {dentist_flags} --threads={params.main_threads} --auxiliary-threads={params.auxiliary_threads} --batch={params.batch_range} - - - - {output} 2> {log}"

rule extend_dentist_config_for_merge_insertions:
    input:
        await_dentist_config(),
        await_pile_ups()
    output:
        temp(dentist_merge_insertions_config_file)
    run:
        merge_config = None
        with open(dentist_config_file, 'r') as base_config:
            merge_config = json.load(base_config)

        if "merge-insertions" not in merge_config:
            merge_config["merge-insertions"] = dict()
        merge_config["merge-insertions"]["insertions"] = insertions
        merge_config["merge-insertions"]["partitioned-insertions"] = list(insertions_batches())

        with open(output[0], 'w') as ext_config_file:
            json.dump(merge_config, ext_config_file)

        validate_dentist_config(output[0])


rule merge_insertions:
    input:
        dentist_merge_insertions_config_file,
        await_db_files(reference),
        await_db_files(reads),
        await_pile_ups(),
        insertions_batches = lambda _: insertions_batches()
    output:
        protected(insertions)
    log: log_file("merge-insertions")
    shell:
        "dentist merge-insertions --config={dentist_merge_insertions_config_file} {dentist_flags} - - 2> {log}"


rule preliminary_output:
    input:
        await_dentist_config(),
        await_db_files(reference),
        insertions
    output:
        preliminary_gap_closed_fasta,
        preliminary_closed_gaps_bed
    params:
        revert_options = "--revert=agp,cache-contig-alignments,scaffolding,skip-gaps,skip-gaps-file"
    log: log_file("preliminary-output")
    shell:
        "dentist output --config={dentist_config_file} {dentist_flags} --closed-gaps-bed={output[1]} {params.revert_options} - - {output[0]} 2> {log}"


rule preliminary_gap_closed_vs_reads_alignment_block:
    input:
        await_dentist_config(),
        await_db_files(preliminary_gap_closed),
        await_db_files(reads),
        mask_files(preliminary_gap_closed, dust_mask),
        mask_files(preliminary_gap_closed, self_mask),
        mask_files(preliminary_gap_closed, tandem_mask)
    output:
        alignment_file(preliminary_gap_closed, reads, block_a='{block_ref}', block_b='{block_reads}'),
        temp(alignment_file(reads, preliminary_gap_closed, block_a='{block_reads}', block_b='{block_ref}'))
    params:
        aligncmd = prepare_flags(lambda : generate_options_for("reads", dentist_config_file)),
        preliminary_gap_closed_stub = db_stub(rel_to_workdir(preliminary_gap_closed)),
        reads_stub = db_stub(rel_to_workdir(reads)),
        alignment_file_a_vs_b = lambda _, output: basename(output[0]),
        alignment_file_b_vs_a = lambda _, output: basename(output[1]),
        dalign_flags = prepare_flags(additional_reads_vs_reads_alignment_options)
    threads: max_threads
    log: log_file("gap-closed-vs-reads-alignment.{block_ref}.{block_reads}")
    shell:
        """
            {{
                cd {workdir_}
                {params.aligncmd} {params.dalign_flags} {params.preliminary_gap_closed_stub}.{wildcards.block_ref} {params.reads_stub}.{wildcards.block_reads}
                LAcheck -v {params.preliminary_gap_closed_stub} {params.reads_stub} {params.alignment_file_a_vs_b} || {lacheck_handler}
                LAcheck -v {params.reads_stub} {params.preliminary_gap_closed_stub} {params.alignment_file_b_vs_a} || {lacheck_handler}
            }} &> {log}
        """


rule preliminary_gap_closed_vs_reads_alignment:
    input:
        await_db_files(preliminary_gap_closed),
        await_db_files(reads),
        block_alignments = lambda _: block_alignments(preliminary_gap_closed, reads)
    output:
        protected(preliminary_gap_closed_vs_reads_alignment)
    log: log_file("gap-closed-vs-reads-alignment")
    shell:
        """
            {{
                LAmerge {output} {input[block_alignments]}
                LAcheck -v {preliminary_gap_closed} {reads} {output} || {lacheck_handler}
            }} &> {log}
        """


rule split_preliminary_gap_closed_vs_reads_alignment:
    input: preliminary_gap_closed_vs_reads_alignment
    output: expand(alignment_file(preliminary_gap_closed, reads, block_a="{block_ref}"), block_ref=range(1, validation_blocks + 1))
    params:
        split_alignment = alignment_file(preliminary_gap_closed, reads, block_a="@")
    log: log_file("split-gap-closed-vs-reads-alignment")
    shell:
        """
            {{
                LAsplit {params.split_alignment} {validation_blocks} < {input}
                LAcheck -v {preliminary_gap_closed} {reads} {output} || {lacheck_handler}
            }} &> {log}
        """


rule preliminary_closed_gaps_bed2mask:
    input:
        db = await_db_files(preliminary_gap_closed),
        bed = preliminary_closed_gaps_bed
    output:
        mask_files(preliminary_gap_closed, closed_gaps_mask)
    log: log_file("closed-gaps-bed2mask")
    shell:
        "dentist bed2mask --config={dentist_config_file} {dentist_flags} --data-comments --bed={input.bed} {input.db[0]} {closed_gaps_mask} 2> {log}"


rule validate_regions_block:
    input:
        refdb = await_db_files(preliminary_gap_closed),
        reads = await_db_files(reads),
        mask_files = mask_files(preliminary_gap_closed, closed_gaps_mask),
        alignment = alignment_file(preliminary_gap_closed, reads, block_a="{block_ref}")
    output:
        temp(validation_report_block),
        temp(mask_files(preliminary_gap_closed, block_mask(weak_coverage_mask, "{block_ref}")))
    params:
        block_mask = block_mask(weak_coverage_mask, "{block_ref}")
    threads: max_threads
    log: log_file("validate-regions-block.{block_ref}")
    shell:
        "dentist validate-regions --config={dentist_config_file} --threads={threads} --weak-coverage-mask={params.block_mask} {input.refdb[0]} - {input.alignment} {closed_gaps_mask} > {output[0]} 2> {log}"


rule validate_regions:
    input:
        block_reports = expand(validation_report_block, block_ref=range(1, validation_blocks + 1))
    output:
        validation_report
    shell:
        "cat {input.block_reports} > {output}"


rule weak_coverage_mask:
    input:
        *expand(mask_files(preliminary_gap_closed, block_mask(weak_coverage_mask, "{block_ref}")), block_ref=range(1, validation_blocks + 1)),
        db = await_db_files(preliminary_gap_closed)
    output:
        mask_files(preliminary_gap_closed, weak_coverage_mask)
    params:
        block_masks = expand(block_mask(weak_coverage_mask, "{block_ref}"), block_ref=range(1, validation_blocks + 1))
    log: log_file("weak-coverage-mask")
    shell:
        "dentist merge-masks --config={dentist_config_file} {dentist_flags} {input.db[0]} {weak_coverage_mask} {params.block_masks} 2> {log}"


rule skip_gaps:
    input: validation_report
    output: skip_gaps
    run:
        failed_validations = None
        with open(validation_report, 'r') as validation_report_file:
            failed_validations = multi_json_load(validation_report_file)

        with open(output[0], 'w') as skip_gaps_file:
            for invalid in failed_validations:
                gap_spec = "-".join((str(cid) for cid in invalid["contigIds"]))
                print(gap_spec, file=skip_gaps_file)


rule purged_output:
    input:
        await_dentist_config(),
        await_db_files(reference),
        insertions,
        skip_gaps = skip_gaps
    output:
        gap_closed_fasta,
        closed_gaps_bed
    log: log_file("purged-output")
    shell:
        "dentist output --config={dentist_config_file} {dentist_flags} --skip-gaps-file={input[skip_gaps]} - - - 2> {log}"


#-----------------------------------------------------------------------------
# END rules
#-----------------------------------------------------------------------------
